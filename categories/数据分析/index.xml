<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>数据分析 on </title>
        <link>/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</link>
        <description>Recent content in 数据分析 on </description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Sun, 22 Oct 2023 00:00:00 +0000</lastBuildDate><atom:link href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>利用Python进行简单的数据分析（1）</title>
        <link>/post/231022_anaylsis_data/</link>
        <pubDate>Sun, 22 Oct 2023 00:00:00 +0000</pubDate>
        
        <guid>/post/231022_anaylsis_data/</guid>
        <description>&lt;p&gt;对三国演义进行简单的数据分析&lt;/p&gt;
&lt;h3 id=&#34;引入必要的库&#34;&gt;引入必要的库&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;import math
import jieba
import collections
import zhconv
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties, fontManager
import wordcloud
from PIL import Image
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;定义清洗函数&#34;&gt;定义清洗函数&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;def clear(s):
    # 繁体转简体
    s = zhconv.convert(s, &#39;zh-hans&#39;)

    # 清除中英文标点和空白符
    for i in &amp;quot; \t\n`~!@#$%^&amp;amp;*()_+{}|\&amp;quot;:?&amp;gt;&amp;lt;,./;&#39;]\\[=-]&#39;，。/；‘！@~【】、=-·~!#￥%……&amp;amp;*（）——+{}|：“《》？&amp;quot;:
        s = s.replace(i,&amp;quot; &amp;quot;)
    return s
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;打开文件&#34;&gt;打开文件&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;# 该文件的编码为utf-16，使用utf-16打开文件并忽视错误的编码。
fp = open(&amp;quot;atotk.txt&amp;quot;,&amp;quot;r&amp;quot;,encoding=&amp;quot;utf-16&amp;quot;,errors=&amp;quot;ignore&amp;quot;);

# 定义计数器
all_counts = collections.Counter()

# 清理不正确的分词
jieba.del_word(&amp;quot;孔明曰&amp;quot;);
jieba.del_word(&amp;quot;玄德曰&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;分词并计数&#34;&gt;分词并计数&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;while True:
    sentence = fp.readline()
    # EOF
    if not sentence:
        break;
    
    sentence = clear(sentence);
    # 分词
    seg_list = jieba.cut(sentence,cut_all=False)
    # 计数
    counts = collections.Counter(list(seg_list))
    all_counts.update(counts)

# 词频最高的词组
gen = [];
for i in all_counts.most_common(500):

    # 对单个汉字进行清理
    if len(i[0]) &amp;gt; 1:
        gen.append(i)

    # 选出出现频率最多的20个词
    if len(gen) &amp;gt; 20:
        break;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;绘制条形图&#34;&gt;绘制条形图&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;plt.figure(dpi=300, figsize=(20,8));

x = [gen[i][0] for i in range(len(gen))]
y = [gen[i][1] for i in range(len(gen))]

# 对于显示汉字异常的问题，引入开源字库Misans
misans = FontProperties(fname=&amp;quot;misans.ttf&amp;quot;,size=12)
fontManager.addfont(&amp;quot;misans.ttf&amp;quot;)
sns.set(font=misans.get_name())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;cubehelix_paltte&lt;/strong&gt;为sns下的画板库，可以生成一系列的颜色值。使用方法为:  &lt;br&gt;
sns.cubehelix_paltte(n, start, rot, reverse, gamma) &lt;br&gt;
&lt;em&gt;@param n: 生成的颜色数量&lt;/em&gt; &lt;br&gt;
&lt;em&gt;@param start,rot: 颜色区间&lt;/em&gt; &lt;br&gt;
&lt;em&gt;@param reverse: 反转生成的颜色序列&lt;/em&gt; &lt;br&gt;
&lt;em&gt;@param gamma: 设置颜色的伽马值，越大颜色越深，默认为1&lt;/em&gt;
&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 绘制条形统计图并美化
fig = sns.barplot(x=x, y=y, palette=sns.cubehelix_palette(
    len(x),start=.75,rot=-.150,reverse=True,gamma=1.5))

# 保存画布
# bbox_inches=&amp;quot;tight&amp;quot;规定了使用较为紧凑的绘图
fig.get_figure().savefig(&amp;quot;o.png&amp;quot;, bbox_inches=&amp;quot;tight&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输出
&lt;img src=&#34;/post/231022_anaylsis_data/o.png&#34;
	width=&#34;4855&#34;
	height=&#34;1993&#34;
	srcset=&#34;/post/231022_anaylsis_data/o_hue671a2dbf39a7bc8fa5172dcad465c7e_106770_480x0_resize_box_3.png 480w, /post/231022_anaylsis_data/o_hue671a2dbf39a7bc8fa5172dcad465c7e_106770_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;s输出&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;243&#34;
		data-flex-basis=&#34;584px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;将数据导出为词云&#34;&gt;将数据导出为词云&lt;/h3&gt;
&lt;p&gt;首先记录词频为dict格式，log(e,n+1)降低词频差
import math
gen = dict();
for i in all_counts.most_common(500):
if len(i[0]) &amp;gt; 1:
gen[i[0]] = int(math.log(i[1]+1))&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 打开遮罩图片
img = Image.open(&amp;quot;cup_icon.png&amp;quot;)
mask = np.array(img)

wc = wordcloud.WordCloud(font_path=&amp;quot;misans.ttf&amp;quot;,
            mask=mask,
            width = 2000,
            height = 2000,
            background_color=&#39;white&#39;,
            max_words=300)
wc = wc.fit_words(gen)

# 显示词云
plt.imshow(wc, interpolation=&#39;bilinear&#39;)
# 不显示坐标轴
plt.axis(&amp;quot;off&amp;quot;)  
# 显示图片
plt.show() 
# 保存到文件
wc.to_file(&amp;quot;o2.png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输出
&lt;img src=&#34;/post/231022_anaylsis_data/o2.png&#34;
	width=&#34;2000&#34;
	height=&#34;2000&#34;
	srcset=&#34;/post/231022_anaylsis_data/o2_hu540f56b010c31d2d3af117104bed47aa_619756_480x0_resize_box_3.png 480w, /post/231022_anaylsis_data/o2_hu540f56b010c31d2d3af117104bed47aa_619756_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;s输出&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
