[{"content":"对三国演义进行简单的数据分析\n引入必要的库 import math import jieba import collections import zhconv import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from matplotlib.font_manager import FontProperties, fontManager import wordcloud from PIL import Image 定义清洗函数 def clear(s): # 繁体转简体 s = zhconv.convert(s, 'zh-hans') # 清除中英文标点和空白符 for i in \u0026quot; \\t\\n`~!@#$%^\u0026amp;*()_+{}|\\\u0026quot;:?\u0026gt;\u0026lt;,./;']\\\\[=-]'，。/；‘！@~【】、=-·~!#￥%……\u0026amp;*（）——+{}|：“《》？\u0026quot;: s = s.replace(i,\u0026quot; \u0026quot;) return s 打开文件 # 该文件的编码为utf-16，使用utf-16打开文件并忽视错误的编码。 fp = open(\u0026quot;atotk.txt\u0026quot;,\u0026quot;r\u0026quot;,encoding=\u0026quot;utf-16\u0026quot;,errors=\u0026quot;ignore\u0026quot;); # 定义计数器 all_counts = collections.Counter() # 清理不正确的分词 jieba.del_word(\u0026quot;孔明曰\u0026quot;); jieba.del_word(\u0026quot;玄德曰\u0026quot;) 分词并计数 while True: sentence = fp.readline() # EOF if not sentence: break; sentence = clear(sentence); # 分词 seg_list = jieba.cut(sentence,cut_all=False) # 计数 counts = collections.Counter(list(seg_list)) all_counts.update(counts) # 词频最高的词组 gen = []; for i in all_counts.most_common(500): # 对单个汉字进行清理 if len(i[0]) \u0026gt; 1: gen.append(i) # 选出出现频率最多的20个词 if len(gen) \u0026gt; 20: break; 绘制条形图 plt.figure(dpi=300, figsize=(20,8)); x = [gen[i][0] for i in range(len(gen))] y = [gen[i][1] for i in range(len(gen))] # 对于显示汉字异常的问题，引入开源字库Misans misans = FontProperties(fname=\u0026quot;misans.ttf\u0026quot;,size=12) fontManager.addfont(\u0026quot;misans.ttf\u0026quot;) sns.set(font=misans.get_name()) cubehelix_paltte为sns下的画板库，可以生成一系列的颜色值。使用方法为: sns.cubehelix_paltte(n, start, rot, reverse, gamma) @param n: 生成的颜色数量 @param start,rot: 颜色区间 @param reverse: 反转生成的颜色序列 @param gamma: 设置颜色的伽马值，越大颜色越深，默认为1 # 绘制条形统计图并美化 fig = sns.barplot(x=x, y=y, palette=sns.cubehelix_palette( len(x),start=.75,rot=-.150,reverse=True,gamma=1.5)) # 保存画布 # bbox_inches=\u0026quot;tight\u0026quot;规定了使用较为紧凑的绘图 fig.get_figure().savefig(\u0026quot;o.png\u0026quot;, bbox_inches=\u0026quot;tight\u0026quot;) 输出 将数据导出为词云 首先记录词频为dict格式，log(e,n+1)降低词频差 import math gen = dict(); for i in all_counts.most_common(500): if len(i[0]) \u0026gt; 1: gen[i[0]] = int(math.log(i[1]+1))\n# 打开遮罩图片 img = Image.open(\u0026quot;cup_icon.png\u0026quot;) mask = np.array(img) wc = wordcloud.WordCloud(font_path=\u0026quot;misans.ttf\u0026quot;, mask=mask, width = 2000, height = 2000, background_color='white', max_words=300) wc = wc.fit_words(gen) # 显示词云 plt.imshow(wc, interpolation='bilinear') # 不显示坐标轴 plt.axis(\u0026quot;off\u0026quot;) # 显示图片 plt.show() # 保存到文件 wc.to_file(\u0026quot;o2.png\u0026quot;) 输出 ","date":"2023-10-22T00:00:00Z","permalink":"/post/231022_anaylsis_data/","title":"利用Python进行简单的数据分析（1）"}]